{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://pranav-pc:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1581250932020)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.Model\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, LinearSVC, OneVsRest}\n",
       "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, MulticlassClassificationEvaluator}\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.ml.feature.Binarizer\n",
       "import org.apache.spark.ml.feature.{RegexTokenizer, NGram}\n",
       "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession;\n",
    "\n",
    "//import statistics.functions._;\n",
    "\n",
    "import org.apache.spark.sql.DataFrame;\n",
    "import org.apache.spark.sql.types._;\n",
    "import org.apache.spark.sql.functions._;\n",
    "import org.apache.spark.ml.Pipeline;\n",
    "import org.apache.spark.ml.Model;\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, LinearSVC, OneVsRest};\n",
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, MulticlassClassificationEvaluator};\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.ml.feature.Binarizer;\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer, NGram};\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF};\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = hdfs://localhost:9000/TextMining/tokens/\n",
       "original_schema: org.apache.spark.sql.types.StructType = StructType(StructField(product,StringType,true), StructField(votes,IntegerType,true), StructField(rate,DoubleType,true), StructField(userID,StringType,true), StructField(original_text,StringType,true), StructField(text,StringType,true), StructField(summary,StringType,true))\n",
       "original_data: org.apache.spark.sql.DataFrame = [product: string, votes: int ... 5 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path: String = \"hdfs://localhost:9000/TextMining/tokens/\";\n",
    "\n",
    "// \"rate\" must be Double because it can be easily binarized by Spark\n",
    "val original_schema = new StructType(Array(\n",
    "  StructField(\"product\",          StringType,  true),\n",
    "  StructField(\"votes\",            IntegerType, true),\n",
    "  StructField(\"rate\",             DoubleType,  true),\n",
    "  StructField(\"userID\",    StringType,  true),\n",
    "  StructField(\"original_text\",    StringType,  true),\n",
    "  StructField(\"text\",             StringType,  true),\n",
    "  StructField(\"summary\",          StringType,  true)));\n",
    "\n",
    "val original_data:DataFrame = spark.read\n",
    "  .options(Map(\"delimiter\" -> \"\\t\"))\n",
    "  .schema(original_schema)\n",
    "  .csv(path)\n",
    "  .na.drop();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+--------------+--------------------+--------------------+--------------------+\n",
      "|   product|votes|rate|        userID|       original_text|                text|             summary|\n",
      "+----------+-----+----+--------------+--------------------+--------------------+--------------------+\n",
      "|0143065971|    1| 5.0|A3JXKC2CMCA9MR|Great condition a...| great condit great |          Five Stars|\n",
      "|0143065971|    4| 5.0| A18GG84RF7J2B|This is a masterp...|masterpiec someon...|Fantastic Book Ab...|\n",
      "|1423600150|    1| 5.0|A2VCOE59QZ9967|Great little cook...|great littl cookb...|           Delicious|\n",
      "|1423600150|    1| 5.0|A1DBZ3TCVMCQW2|   Great mexi stuff.|   great mexi stuff |          Five Stars|\n",
      "|1423600150|    1| 5.0| AYXIWMJSXU3PW|          Great book|              great |          Five Stars|\n",
      "+----------+-----+----+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* original_data.select(\"summary\",\"rate\",\"votes\")\n",
    " *    .groupBy(\"summary\")\n",
    " *    .agg(mean(\"rate\"), mean(\"votes\"), count(\"summary\"))\n",
    " *   .orderBy(desc(\"count(summary)\"))\n",
    " */"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binarizer: org.apache.spark.ml.feature.Binarizer = Binarizer: uid=binarizer_3e35340c4b7f\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_65d53eba131b\n",
       "ngrams: org.apache.spark.ml.feature.NGram = NGram: uid=ngram_23d490d70b8f, n=2\n",
       "tf: org.apache.spark.ml.feature.HashingTF = HashingTF: uid=hashingTF_cca4b7701211, binary=false, numFeatures=262144\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_be39df358ffb\n",
       "classifierMod: org.apache.spark.ml.classification.LogisticRegression = logreg_ee64752c37b0\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_f2832bcc437b\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tngram_23d490d70b8f-n: 1,\n",
       "\tlogreg_ee64752c37b0-regParam: 0.01\n",
       "}, {\n",
       "\tngram_23d490d70b8f-n: 2,\n",
       "\tlogreg_ee64752c37b0-regParam: 0.01\n",
       "}, {\n",
       "\tngram_23d490d70b8f-n: 3,\n",
       "\tl...\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val binarizer = new Binarizer()\n",
    "  .setInputCol(\"rate\")\n",
    "  .setOutputCol(\"label\")\n",
    "  .setThreshold(3.5);\n",
    "\n",
    "// get n-grams\n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"tokens\")\n",
    "  .setPattern(\"\\\\W\");\n",
    "val ngrams = new NGram()\n",
    "  .setInputCol(tokenizer.getOutputCol)\n",
    "  .setOutputCol(\"n-grams\");\n",
    "\n",
    "// calc tf-idf \n",
    "val tf = new HashingTF()\n",
    "  .setInputCol(ngrams.getOutputCol)\n",
    "  .setOutputCol(\"tf\");\n",
    "val idf = new IDF()\n",
    "  .setInputCol(tf.getOutputCol)\n",
    "  .setOutputCol(\"tf-idf\")\n",
    "  .setMinDocFreq(3);\n",
    "\n",
    "// build the classifier\n",
    "val classifierMod = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setFeaturesCol(idf.getOutputCol)\n",
    "  .setLabelCol(binarizer.getOutputCol);\n",
    "\n",
    "// this is the pipeline that data follows to be evaluated\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(binarizer, tokenizer, ngrams, tf, idf, classifierMod));\n",
    "\n",
    "// a little of optimization: try different hyperparameters\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(classifierMod.regParam, Array(0.01, 0.05, 0.1))\n",
    "  .addGrid(ngrams.n, Array(1, 2, 3))\n",
    "  .build();\n",
    "\n",
    "// do it with a cross validation on the train set (3 folds)\n",
    "val evaluator = new MulticlassClassificationEvaluator() //Ã¨ f1 sulla classe 0.0\n",
    "\n",
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "// spark.sql(\"SELECT summary, COUNT(*) FROM original_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... \n",
      "done!\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_23d490d70b8f-n: 1,\n",
      "\tlogreg_ee64752c37b0-regParam: 0.01\n",
      "}\n",
      "0.8767617577593136\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_23d490d70b8f-n: 2,\n",
      "\tlogreg_ee64752c37b0-regParam: 0.01\n",
      "}\n",
      "0.8845783557809365\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_23d490d70b8f-n: 3,\n",
      "\tlogreg_ee64752c37b0-regParam: 0.01\n",
      "}\n",
      "0.813688775592737\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_23d490d70b8f-n: 1,\n",
      "\tlogreg_ee64752c37b0-regParam: 0.05\n",
      "}\n",
      "0.8564910958427848\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_23d490d70b8f-n: 2,\n",
      "\tlogreg_ee64752c37b0-regParam: 0.05\n",
      "}\n",
      "0.8727693110974878\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_23d490d70b8f-n: 3,\n",
      "\tlogreg_ee64752c37b0-regParam: 0.05\n",
      "}\n",
      "0.8037707567982274\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_23d490d70b8f-n: 1,\n",
      "\tlogreg_ee64752c37b0-regParam: 0.1\n",
      "}\n",
      "0.8405928493000251\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_23d490d70b8f-n: 2,\n",
      "\tlogreg_ee64752c37b0-regParam: 0.1\n",
      "}\n",
      "0.860296788185761\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_23d490d70b8f-n: 3,\n",
      "\tlogreg_ee64752c37b0-regParam: 0.1\n",
      "}\n",
      "0.7933461211257044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.tuning.CrossValidatorModel = CrossValidatorModel: uid=cv_3756532dad18, bestModel=pipeline_f2832bcc437b, numFolds=3\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Training... \");\n",
    "val model = cv.fit(original_data);\n",
    "println(\"done!\");\n",
    "\n",
    "// print results\n",
    "for (i <- 0 until model.avgMetrics.size) {\n",
    "  println(\"\\n\\n\");\n",
    "  println(model.getEstimatorParamMaps(i));\n",
    "  println(model.avgMetrics(i));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM multiclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_fac8c4b89e51\n",
       "ngrams: org.apache.spark.ml.feature.NGram = NGram: uid=ngram_84dd183c53be, n=2\n",
       "tf: org.apache.spark.ml.feature.HashingTF = HashingTF: uid=hashingTF_98d8d6790dfb, binary=false, numFeatures=262144\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_b4687644f064\n",
       "svm: org.apache.spark.ml.classification.LinearSVC = linearsvc_dfa2ea2e229c\n",
       "classifierMod: org.apache.spark.ml.classification.OneVsRest = oneVsRest_5ea4752e8604\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_2493a742fb86\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tngram_84dd183c53be-n: 1,\n",
       "\tlinearsvc_dfa2ea2e229c-regParam: 0.1\n",
       "}, {\n",
       "\tngram_84dd183c53be-n: 1,\n",
       "\tlinearsvc_dfa2ea2e229c-regParam: 0.5\n",
       "}, {\n",
       "\tngram_84dd183c53be-n: 2,\n",
       "\tlinearsvc_dfa2ea2...\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get n-grams\n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"tokens\")\n",
    "  .setPattern(\"\\\\W\");\n",
    "val ngrams = new NGram()\n",
    "  .setInputCol(tokenizer.getOutputCol)\n",
    "  .setOutputCol(\"n-grams\");\n",
    "\n",
    "// calc tf-idf \n",
    "val tf = new HashingTF()\n",
    "  .setInputCol(ngrams.getOutputCol)\n",
    "  .setOutputCol(\"tf\");\n",
    "val idf = new IDF()\n",
    "  .setInputCol(tf.getOutputCol)\n",
    "  .setOutputCol(\"tf-idf\")\n",
    "  .setMinDocFreq(3);\n",
    "\n",
    "// build the classifier\n",
    "val svm = new LinearSVC();\n",
    "\n",
    "val classifierMod = new OneVsRest()\n",
    "  .setClassifier(svm)\n",
    "  .setFeaturesCol(idf.getOutputCol)\n",
    "  .setLabelCol(\"rate\");\n",
    "\n",
    "// this is the pipeline that data follows to be evaluated\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer, ngrams, tf, idf, classifierMod));\n",
    "\n",
    "// a little of optimization: try different hyperparameters\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(svm.regParam, Array(0.1, 0.5))\n",
    "  .addGrid(ngrams.n, Array(1, 2, 3))\n",
    "  .build();\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "                        .setMetricName(\"accuracy\")\n",
    "                        .setLabelCol(\"rate\")\n",
    "\n",
    "// do it with a cross validation on the train set (3 folds)\n",
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... done!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.tuning.CrossValidatorModel = CrossValidatorModel: uid=cv_9bd91ae77766, bestModel=pipeline_2493a742fb86, numFolds=3\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training... \");\n",
    "val model = cv.fit(original_data)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_84dd183c53be-n: 1,\n",
      "\tlinearsvc_dfa2ea2e229c-regParam: 0.1\n",
      "}\n",
      "0.6444849701057255\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_84dd183c53be-n: 1,\n",
      "\tlinearsvc_dfa2ea2e229c-regParam: 0.5\n",
      "}\n",
      "0.6252414319347207\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_84dd183c53be-n: 2,\n",
      "\tlinearsvc_dfa2ea2e229c-regParam: 0.1\n",
      "}\n",
      "0.6603150472436443\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_84dd183c53be-n: 2,\n",
      "\tlinearsvc_dfa2ea2e229c-regParam: 0.5\n",
      "}\n",
      "0.6402047012108124\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_84dd183c53be-n: 3,\n",
      "\tlinearsvc_dfa2ea2e229c-regParam: 0.1\n",
      "}\n",
      "0.6199566142785332\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_84dd183c53be-n: 3,\n",
      "\tlinearsvc_dfa2ea2e229c-regParam: 0.5\n",
      "}\n",
      "0.6139563253890734\n"
     ]
    }
   ],
   "source": [
    "for (i <- 0 until model.avgMetrics.size) {\n",
    "  println(\"\\n\\n\");\n",
    "  println(model.getEstimatorParamMaps(i));\n",
    "  println(model.avgMetrics(i));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|rate|  count|\n",
      "+----+-------+\n",
      "| 1.0| 258294|\n",
      "| 4.0|1197765|\n",
      "| 3.0| 475678|\n",
      "| 2.0| 213119|\n",
      "| 5.0|3319659|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_data.groupBy(\"rate\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [product: string, votes: int ... 10 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = model.bestModel.transform(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionsAndLabels: org.apache.spark.sql.Dataset[(Double, Double)] = [_1: double, _2: double]\n",
       "metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@5d7d9091\n",
       "confusionMatrix: org.apache.spark.mllib.linalg.Matrix =\n",
       "125244.0  3577.0   7820.0    2816.0    118816.0\n",
       "31162.0   47076.0  24203.0   8216.0    102431.0\n",
       "14199.0   6061.0   134585.0  49938.0   270750.0\n",
       "4772.0    2617.0   26373.0   207166.0  956442.0\n",
       "4118.0    1505.0   8109.0    42067.0   3263300.0\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionsAndLabels = results.select(\"prediction\", \"rate\")\n",
    "  .map(row => (row.getDouble(0), row.getDouble(1)))\n",
    "\n",
    "val metrics = new MulticlassMetrics(predictionsAndLabels.rdd)\n",
    "\n",
    "val confusionMatrix = metrics.confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 measure  0.6289661678651075\n",
      "weightedRecall  0.6912545761151722\n",
      "weightedPrecision  0.6885475253460855\n"
     ]
    }
   ],
   "source": [
    "println(s\"F1 measure  ${metrics.weightedFMeasure}\")\n",
    "\n",
    "println(s\"weightedRecall  ${metrics.weightedRecall}\")\n",
    "\n",
    "println(s\"weightedPrecision  ${metrics.weightedPrecision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: org.apache.spark.mllib.linalg.Matrix =\n",
       "125244.0  3577.0   7820.0    2816.0    118816.0\n",
       "31162.0   47076.0  24203.0   8216.0    102431.0\n",
       "14199.0   6061.0   134585.0  49938.0   270750.0\n",
       "4772.0    2617.0   26373.0   207166.0  956442.0\n",
       "4118.0    1505.0   8109.0    42067.0   3263300.0\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "confusionMatrix = np.array([[125244.0, 3577.0,  7820.0,   2816.0,   118816.0],\n",
    "                            [31162.0,  47076.0, 24203.0,  8216.0,   102431.0],\n",
    "                            [14199.0,  6061.0,  134585.0, 49938.0,  270750.0],\n",
    "                            [4772.0,   2617.0,  26373.0,  207166.0, 956442.0],\n",
    "                            [4118.0,   1505.0,  8109.0,   42067.0,  3263300.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "#sort of precision\n",
    "sns.heatmap(confusionMatrix/np.sum(confusionMatrix, axis=0), annot=True, cmap=\"Blues\",\n",
    "           xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "#sort of recall\n",
    "sns.heatmap((confusionMatrix.swapaxes(0,1)/np.sum(confusionMatrix, axis=1)).swapaxes(0,1), \n",
    "            annot=True, cmap=\"Blues\",\n",
    "           xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "sns.heatmap(confusionMatrix, annot=True, cmap=\"Blues\",\n",
    "           xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "name": "Regression_scala.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
