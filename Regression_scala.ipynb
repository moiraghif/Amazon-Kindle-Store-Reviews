{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://pranav-pc:4041\n",
       "SparkContext available as 'sc' (version = 3.0.0-preview2, master = local[*], app id = local-1580823277376)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.Model\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.Binarizer\n",
       "import org.apache.spark.ml.feature.{RegexTokenizer, NGram}\n",
       "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession;\n",
    "\n",
    "//import statistics.functions._;\n",
    "\n",
    "import org.apache.spark.sql.DataFrame;\n",
    "import org.apache.spark.sql.types._;\n",
    "import org.apache.spark.sql.functions._;\n",
    "import org.apache.spark.ml.Pipeline;\n",
    "import org.apache.spark.ml.Model;\n",
    "import org.apache.spark.ml.classification.LogisticRegression;\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator;\n",
    "import org.apache.spark.ml.feature.Binarizer;\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer, NGram};\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF};\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = hdfs://localhost:9000/TextMining/tokens/part-00000\n",
       "original_schema: org.apache.spark.sql.types.StructType = StructType(StructField(product,StringType,true), StructField(votes,IntegerType,true), StructField(rate,DoubleType,true), StructField(original_text,StringType,true), StructField(text,StringType,true), StructField(summary,StringType,true))\n",
       "original_data: org.apache.spark.sql.DataFrame = [product: string, votes: int ... 4 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path: String = \"hdfs://localhost:9000/TextMining/tokens/part-00000\";\n",
    "\n",
    "// \"rate\" must be Double because it can be easily binarized by Spark\n",
    "val original_schema = new StructType(Array(\n",
    "  StructField(\"product\", StringType,  true),\n",
    "  StructField(\"votes\",   IntegerType, true),\n",
    "  StructField(\"rate\",    DoubleType,  true),\n",
    "  StructField(\"original_text\",    StringType,  true),\n",
    "  StructField(\"text\",    StringType,  true),\n",
    "  StructField(\"summary\",    StringType,  true)));\n",
    "\n",
    "val original_data:DataFrame = spark.read\n",
    "  .options(Map(\"delimiter\" -> \"\\t\"))\n",
    "  .schema(original_schema)\n",
    "  .csv(path)\n",
    "  .na.drop();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+--------------------+--------------------+--------------------+\n",
      "|   product|votes|rate|       original_text|                text|             summary|\n",
      "+----------+-----+----+--------------------+--------------------+--------------------+\n",
      "|0143065971|    4| 5.0|This is a masterp...|masterpiec someon...|Fantastic Book Ab...|\n",
      "|0143065971|    1| 5.0|Great condition a...| great condit great |          Five Stars|\n",
      "|1423600150|    1| 5.0|Excellent book on...|excel sauc fun tr...|           excellent|\n",
      "|1423600150|    1| 5.0|          Great book|              great |          Five Stars|\n",
      "|1423600150|    1| 5.0|   Great mexi stuff.|   great mexi stuff |          Five Stars|\n",
      "+----------+-----+----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------------+--------------+\n",
      "|    summary|         avg(rate)|        avg(votes)|count(summary)|\n",
      "+-----------+------------------+------------------+--------------+\n",
      "| Five Stars| 4.999868593955322|1.0273981603153746|         15220|\n",
      "| Four Stars| 4.000463714351959|1.0192441456063066|          4313|\n",
      "|Three Stars| 3.001019367991845|1.0341488277268094|          1962|\n",
      "|   One Star|1.0051380860629415|1.2190109184328837|          1557|\n",
      "| Great book| 4.845056065239551|1.2252803261977574|           981|\n",
      "|  Good read| 4.265864332603939|1.1816192560175054|           914|\n",
      "|  Two Stars|2.0033632286995515|1.0661434977578474|           892|\n",
      "| Great read| 4.753731343283582|1.2624378109452736|           804|\n",
      "|      Great| 4.789115646258503|1.1319727891156464|           735|\n",
      "|       Good| 4.152447552447552|1.1272727272727272|           715|\n",
      "|    Awesome| 4.898697539797395|1.1331403762662808|           691|\n",
      "|  Excellent|  4.92515923566879|1.5398089171974523|           628|\n",
      "|  Good book|4.3381877022653725|1.0857605177993528|           618|\n",
      "| Great Book| 4.867857142857143|1.4785714285714286|           560|\n",
      "| Great Read| 4.796190476190477| 1.539047619047619|           525|\n",
      "|  Loved it!| 4.886100386100386| 1.694980694980695|           518|\n",
      "|    Amazing| 4.932038834951456|1.4252427184466019|           515|\n",
      "|   Loved it| 4.870614035087719| 1.111842105263158|           456|\n",
      "|Great book!| 4.884353741496598| 1.342403628117914|           441|\n",
      "|Interesting| 3.872685185185185|1.3356481481481481|           432|\n",
      "+-----------+------------------+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_data.select(\"summary\",\"rate\",\"votes\").groupBy(\"summary\").agg(mean(\"rate\"),mean(\"votes\"), count(\"summary\")).orderBy(desc(\"count(summary)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binarizer: org.apache.spark.ml.feature.Binarizer = Binarizer: uid=binarizer_1f2ecfd8bb5d\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_4eca11e23721\n",
       "ngrams: org.apache.spark.ml.feature.NGram = NGram: uid=ngram_08dd240d9a73, n=2\n",
       "tf: org.apache.spark.ml.feature.HashingTF = HashingTF: uid=hashingTF_1ed4df629fa6, binary=false, numFeatures=262144\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_6db681b5f222\n",
       "classifierMod: org.apache.spark.ml.classification.LogisticRegression = logreg_9cdc54499216\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_0234d6e5c92e\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tngram_08dd240d9a73-n: 1,\n",
       "\tlogreg_9cdc54499216-regParam: 0.01\n",
       "}, {\n",
       "\tngram_08dd240d9a73-n: 1,\n",
       "\tlogreg_9cdc54499216-regParam: 0.05\n",
       "}, {\n",
       "\tngram_08dd240d9a73-n: 1,\n",
       "\tl...\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val binarizer = new Binarizer()\n",
    "  .setInputCol(\"rate\")\n",
    "  .setOutputCol(\"label\")\n",
    "  .setThreshold(3.5);\n",
    "\n",
    "// get n-grams\n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"tokens\")\n",
    "  .setPattern(\"\\\\W\");\n",
    "val ngrams = new NGram()\n",
    "  .setInputCol(tokenizer.getOutputCol)\n",
    "  .setOutputCol(\"n-grams\");\n",
    "\n",
    "// calc tf-idf \n",
    "val tf = new HashingTF()\n",
    "  .setInputCol(ngrams.getOutputCol)\n",
    "  .setOutputCol(\"tf\");\n",
    "val idf = new IDF()\n",
    "  .setInputCol(tf.getOutputCol)\n",
    "  .setOutputCol(\"tf-idf\")\n",
    "  .setMinDocFreq(3);\n",
    "\n",
    "// build the classifier\n",
    "val classifierMod = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setFeaturesCol(idf.getOutputCol)\n",
    "  .setLabelCol(binarizer.getOutputCol);\n",
    "\n",
    "// this is the pipeline that data follows to be evaluated\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(binarizer, tokenizer, ngrams, tf, idf, classifierMod));\n",
    "\n",
    "// a little of optimization: try different hyperparameters\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(classifierMod.regParam, Array(0.01, 0.05, 0.1))\n",
    "  .addGrid(ngrams.n, Array(1, 2, 3))\n",
    "  .build();\n",
    "\n",
    "// do it with a cross validation on the train set (3 folds)\n",
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(new BinaryClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " grouping expressions sequence is empty, and 'original_data.`summary`' is not an aggregate function. Wrap '(count(1) AS `count(1)`)' in windowing function(s) or wrap 'original_data.`summary`' in first() (or first_value) if you don't care which value you get.;;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: grouping expressions sequence is empty, and 'original_data.`summary`' is not an aggregate function. Wrap '(count(1) AS `count(1)`)' in windowing function(s) or wrap 'original_data.`summary`' in first() (or first_value) if you don't care which value you get.;;",
      "Aggregate [summary#5, count(1) AS count(1)#182L]",
      "+- SubqueryAlias `original_data`",
      "   +- Filter AtLeastNNulls(n, product#0,votes#1,rate#2,original_text#3,text#4,summary#5)",
      "      +- RelationV2[product#0, votes#1, rate#2, original_text#3, text#4, summary#5] csv hdfs://localhost:9000/TextMining/tokens/part-00000",
      "",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:48)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:47)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:122)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:246)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13(CheckAnalysis.scala:282)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13$adapted(CheckAnalysis.scala:282)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:282)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:91)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:91)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:88)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:69)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:66)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)",
      "  ... 41 elided",
      ""
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT summary, COUNT(*) FROM Original_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... \n",
      "done!\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_08dd240d9a73-n: 1,\n",
      "\tlogreg_9cdc54499216-regParam: 0.01\n",
      "}\n",
      "0.894217505803825\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_08dd240d9a73-n: 1,\n",
      "\tlogreg_9cdc54499216-regParam: 0.05\n",
      "}\n",
      "0.9034031006521369\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_08dd240d9a73-n: 1,\n",
      "\tlogreg_9cdc54499216-regParam: 0.1\n",
      "}\n",
      "0.905312185591742\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_08dd240d9a73-n: 2,\n",
      "\tlogreg_9cdc54499216-regParam: 0.01\n",
      "}\n",
      "0.8443155195102902\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_08dd240d9a73-n: 2,\n",
      "\tlogreg_9cdc54499216-regParam: 0.05\n",
      "}\n",
      "0.8644238243016412\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_08dd240d9a73-n: 2,\n",
      "\tlogreg_9cdc54499216-regParam: 0.1\n",
      "}\n",
      "0.872029019745533\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_08dd240d9a73-n: 3,\n",
      "\tlogreg_9cdc54499216-regParam: 0.01\n",
      "}\n",
      "0.6673848921888594\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_08dd240d9a73-n: 3,\n",
      "\tlogreg_9cdc54499216-regParam: 0.05\n",
      "}\n",
      "0.6837430757007262\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_08dd240d9a73-n: 3,\n",
      "\tlogreg_9cdc54499216-regParam: 0.1\n",
      "}\n",
      "0.6908928013842282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.tuning.CrossValidatorModel = CrossValidatorModel: uid=cv_067743676650, bestModel=pipeline_0234d6e5c92e, numFolds=3\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Training... \");\n",
    "val model = cv.fit(original_data);\n",
    "println(\"done!\");\n",
    "\n",
    "// print results\n",
    "for (i <- 0 until model.avgMetrics.size) {\n",
    "  println(\"\\n\\n\");\n",
    "  println(model.getEstimatorParamMaps(i));\n",
    "  println(model.avgMetrics(i));\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
