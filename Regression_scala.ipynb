{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://beast-pc.homenet.telecomitalia.it:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0-preview2, master = local[*], app id = local-1580916626780)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.Model\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.Binarizer\n",
       "import org.apache.spark.ml.feature.{RegexTokenizer, NGram}\n",
       "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession;\n",
    "\n",
    "//import statistics.functions._;\n",
    "\n",
    "import org.apache.spark.sql.DataFrame;\n",
    "import org.apache.spark.sql.types._;\n",
    "import org.apache.spark.sql.functions._;\n",
    "import org.apache.spark.ml.Pipeline;\n",
    "import org.apache.spark.ml.Model;\n",
    "import org.apache.spark.ml.classification.LogisticRegression;\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator;\n",
    "import org.apache.spark.ml.feature.Binarizer;\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer, NGram};\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF};\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = hdfs://localhost:9000/TextMining/tokens/\n",
       "original_schema: org.apache.spark.sql.types.StructType = StructType(StructField(product,StringType,true), StructField(votes,IntegerType,true), StructField(rate,DoubleType,true), StructField(original_text,StringType,true), StructField(text,StringType,true), StructField(summary,StringType,true))\n",
       "original_data: org.apache.spark.sql.DataFrame = [product: string, votes: int ... 4 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path: String = \"hdfs://localhost:9000/TextMining/tokens/\";\n",
    "\n",
    "// \"rate\" must be Double because it can be easily binarized by Spark\n",
    "val original_schema = new StructType(Array(\n",
    "  StructField(\"product\",          StringType,  true),\n",
    "  StructField(\"votes\",            IntegerType, true),\n",
    "  StructField(\"rate\",             DoubleType,  true),\n",
    "  StructField(\"original_text\",    StringType,  true),\n",
    "  StructField(\"text\",             StringType,  true),\n",
    "  StructField(\"summary\",          StringType,  true)));\n",
    "\n",
    "val original_data:DataFrame = spark.read\n",
    "  .options(Map(\"delimiter\" -> \"\\t\"))\n",
    "  .schema(original_schema)\n",
    "  .csv(path)\n",
    "  .na.drop();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+--------------------+--------------------+--------------------+\n",
      "|   product|votes|rate|       original_text|                text|             summary|\n",
      "+----------+-----+----+--------------------+--------------------+--------------------+\n",
      "|0143065971|    4| 5.0|This is a masterp...|masterpiec someon...|Fantastic Book Ab...|\n",
      "|0143065971|    1| 5.0|Great condition a...| great condit great |          Five Stars|\n",
      "|1423600150|    1| 5.0|Excellent book on...|excel sauc fun tr...|           excellent|\n",
      "|1423600150|    1| 5.0|          Great book|              great |          Five Stars|\n",
      "|1423600150|    1| 5.0|   Great mexi stuff.|   great mexi stuff |          Five Stars|\n",
      "+----------+-----+----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/* original_data.select(\"summary\",\"rate\",\"votes\")\n",
    " *    .groupBy(\"summary\")\n",
    " *    .agg(mean(\"rate\"), mean(\"votes\"), count(\"summary\"))\n",
    " *   .orderBy(desc(\"count(summary)\"))\n",
    " */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binarizer: org.apache.spark.ml.feature.Binarizer = Binarizer: uid=binarizer_f6c1a0c2d18d\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_c5e5c52b6619\n",
       "ngrams: org.apache.spark.ml.feature.NGram = NGram: uid=ngram_5145290d4b78, n=2\n",
       "tf: org.apache.spark.ml.feature.HashingTF = HashingTF: uid=hashingTF_3d6a0d0708ea, binary=false, numFeatures=262144\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_c79ca263202a\n",
       "classifierMod: org.apache.spark.ml.classification.LogisticRegression = logreg_337cbecdac6b\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_0abfdf22723d\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tngram_5145290d4b78-n: 1,\n",
       "\tlogreg_337cbecdac6b-regParam: 0.01\n",
       "}, {\n",
       "\tngram_5145290d4b78-n: 2,\n",
       "\tlogreg_337cbecdac6b-regParam: 0.01\n",
       "}, {\n",
       "\tngram_5145290d4b78-n: 3,\n",
       "\tl...\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val binarizer = new Binarizer()\n",
    "  .setInputCol(\"rate\")\n",
    "  .setOutputCol(\"label\")\n",
    "  .setThreshold(3.5);\n",
    "\n",
    "// get n-grams\n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"tokens\")\n",
    "  .setPattern(\"\\\\W\");\n",
    "val ngrams = new NGram()\n",
    "  .setInputCol(tokenizer.getOutputCol)\n",
    "  .setOutputCol(\"n-grams\");\n",
    "\n",
    "// calc tf-idf \n",
    "val tf = new HashingTF()\n",
    "  .setInputCol(ngrams.getOutputCol)\n",
    "  .setOutputCol(\"tf\");\n",
    "val idf = new IDF()\n",
    "  .setInputCol(tf.getOutputCol)\n",
    "  .setOutputCol(\"tf-idf\")\n",
    "  .setMinDocFreq(3);\n",
    "\n",
    "// build the classifier\n",
    "val classifierMod = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setFeaturesCol(idf.getOutputCol)\n",
    "  .setLabelCol(binarizer.getOutputCol);\n",
    "\n",
    "// this is the pipeline that data follows to be evaluated\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(binarizer, tokenizer, ngrams, tf, idf, classifierMod));\n",
    "\n",
    "// a little of optimization: try different hyperparameters\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(classifierMod.regParam, Array(0.01, 0.05, 0.1))\n",
    "  .addGrid(ngrams.n, Array(1, 2, 3))\n",
    "  .build();\n",
    "\n",
    "// do it with a cross validation on the train set (3 folds)\n",
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(new BinaryClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// spark.sql(\"SELECT summary, COUNT(*) FROM original_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "\tngram_5145290d4b78-n: 1,\n",
      "\tlogreg_337cbecdac6b-regParam: 0.01\n",
      "}\n",
      "0.9019483361008155\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_5145290d4b78-n: 2,\n",
      "\tlogreg_337cbecdac6b-regParam: 0.01\n",
      "}\n",
      "0.8985675928328255\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_5145290d4b78-n: 3,\n",
      "\tlogreg_337cbecdac6b-regParam: 0.01\n",
      "}\n",
      "0.7627563338905388\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\tngram_5145290d4b78-n: 1,\n",
      "\tlogreg_337cbecdac6b-regParam: 0.05\n",
      "}\n",
      "0.9036257241517349\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_5145290d4b78-n: 2,\n",
      "\tlogreg_337cbecdac6b-regParam: 0.05\n",
      "}\n",
      "0.9044877488592841\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_5145290d4b78-n: 3,\n",
      "\tlogreg_337cbecdac6b-regParam: 0.05\n",
      "}\n",
      "0.7693852143472046\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_5145290d4b78-n: 1,\n",
      "\tlogreg_337cbecdac6b-regParam: 0.1\n",
      "}\n",
      "0.9034636511888169\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_5145290d4b78-n: 2,\n",
      "\tlogreg_337cbecdac6b-regParam: 0.1\n",
      "}\n",
      "0.906886374283741\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "\tngram_5145290d4b78-n: 3,\n",
      "\tlogreg_337cbecdac6b-regParam: 0.1\n",
      "}\n",
      "0.7727091640831082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.tuning.CrossValidatorModel = CrossValidatorModel: uid=cv_02ece6023b0e, bestModel=pipeline_0abfdf22723d, numFolds=3\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Training... \");\n",
    "val model = cv.fit(original_data);\n",
    "println(\"done!\");\n",
    "\n",
    "// print results\n",
    "for (i <- 0 until model.avgMetrics.size) {\n",
    "  println(\"\\n\\n\");\n",
    "  println(model.getEstimatorParamMaps(i));\n",
    "  println(model.avgMetrics(i));\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/fede/.anaconda/envs/textmining/bin/python",
    "-m",
    "spylon_kernel",
    "-f",
    "{connection_file}"
   ],
   "display_name": "spylon-kernel",
   "env": {
    "PYTHONUNBUFFERED": "1",
    "SPARK_SUBMIT_OPTS": "-Dscala.usejavacp=true"
   },
   "interrupt_mode": "signal",
   "language": "scala",
   "metadata": null,
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": {
    "text": [
     "MetaKernel Magics",
     "url",
     "https://metakernel.readthedocs.io/en/latest/source/README.html"
    ]
   },
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "name": "Regression_scala.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
